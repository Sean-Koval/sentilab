{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda784a393941bc4e35b7c87680a7ceede7",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "---\n",
    "# Scrapping Twitter for Company Sentiment\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Steps for analyzing Twitter data to determine sentiment towards companies\n",
    "\n",
    "* Scrape Tweets over N-period\n",
    "* Add relevance of financial institutions\n",
    "* Use LDA to extract relevant topics from the Tweets\n",
    "* LSTM (deep learning) for sentiment extraction\n",
    "* NER tagger (Stanford) used for enittyu recognition (Organization, Location, Names)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Scrape Tweet Data\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# NTLK functions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize as tok\n",
    "from nltk.stem.snowball import SnowballStemmer # load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "import lda # topic modeling -NMF & LDA\n",
    "import string\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "# Tf-Idf and Clustering packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "source": [
    "## Read in Tweet data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_raw = pd.read_csv('../../data/data_all_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['mortgage','current account','savings account','insurance','credit card','pension',\n",
    "                'personal loan','money transfer','tax advice','investment','wealth management']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "tweet_df_all.shape"
   ]
  },
  {
   "source": [
    "## Add List of Financial Institutions providing afformentioned products\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_raw = tweet_df_raw[tweet_df_raw['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(43704, 13)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    id            author_id  \\\n",
       "0  1178457108276289536             40080176   \n",
       "1  1178455823242035201  1126071201481334787   \n",
       "2  1178450126219685893   729387514914603009   \n",
       "3  1178446295985541120           1697126574   \n",
       "4  1178446170722619393           1239955070   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  This normalisation of no deal is horrendous. P...         0   \n",
       "1  Jumbo Mortgage Program https:// conclud.com/ht...         0   \n",
       "2  If you have no work it's harder to feed your k...         0   \n",
       "3  Solution. \"You'll need to be: 18+ and a UK res...         0   \n",
       "4  Kabaddi x3 UK Premier 1st show House Full Show...         0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/KatarinaKeys/status/117845...   \n",
       "1  https://twitter.com/Conclud2/status/1178455823...   \n",
       "2  https://twitter.com/cjhenrygonzo/status/117845...   \n",
       "3  https://twitter.com/blazedstorm/status/1178446...   \n",
       "4  https://twitter.com/habamoment/status/11784461...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-29 23:50:43+00:00  Sun Sep 29 23:50:43 +0000 2019          0   \n",
       "1  2019-09-29 23:45:37+00:00  Sun Sep 29 23:45:37 +0000 2019          0   \n",
       "2  2019-09-29 23:22:59+00:00  Sun Sep 29 23:22:59 +0000 2019          0   \n",
       "3  2019-09-29 23:07:46+00:00  Sun Sep 29 23:07:46 +0000 2019          2   \n",
       "4  2019-09-29 23:07:16+00:00  Sun Sep 29 23:07:16 +0000 2019          0   \n",
       "\n",
       "  mentions hashtags  geo                                               urls  \\\n",
       "0      NaN      NaN  NaN                                                NaN   \n",
       "1      NaN      NaN  NaN  https://conclud.com/https-www-madisonmortgageg...   \n",
       "2      NaN      NaN  NaN                                                NaN   \n",
       "3      NaN      NaN  NaN                                                NaN   \n",
       "4  @Peepal      NaN  NaN  https://www.facebook.com/habteam/posts/1106547...   \n",
       "\n",
       "  search_term  \n",
       "0    mortgage  \n",
       "1    mortgage  \n",
       "2    mortgage  \n",
       "3    mortgage  \n",
       "4    mortgage  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>author_id</th>\n      <th>text</th>\n      <th>retweets</th>\n      <th>permalink</th>\n      <th>date</th>\n      <th>formatted_date</th>\n      <th>favorites</th>\n      <th>mentions</th>\n      <th>hashtags</th>\n      <th>geo</th>\n      <th>urls</th>\n      <th>search_term</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1178457108276289536</td>\n      <td>40080176</td>\n      <td>This normalisation of no deal is horrendous. P...</td>\n      <td>0</td>\n      <td>https://twitter.com/KatarinaKeys/status/117845...</td>\n      <td>2019-09-29 23:50:43+00:00</td>\n      <td>Sun Sep 29 23:50:43 +0000 2019</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>mortgage</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1178455823242035201</td>\n      <td>1126071201481334787</td>\n      <td>Jumbo Mortgage Program https:// conclud.com/ht...</td>\n      <td>0</td>\n      <td>https://twitter.com/Conclud2/status/1178455823...</td>\n      <td>2019-09-29 23:45:37+00:00</td>\n      <td>Sun Sep 29 23:45:37 +0000 2019</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>https://conclud.com/https-www-madisonmortgageg...</td>\n      <td>mortgage</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1178450126219685893</td>\n      <td>729387514914603009</td>\n      <td>If you have no work it's harder to feed your k...</td>\n      <td>0</td>\n      <td>https://twitter.com/cjhenrygonzo/status/117845...</td>\n      <td>2019-09-29 23:22:59+00:00</td>\n      <td>Sun Sep 29 23:22:59 +0000 2019</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>mortgage</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1178446295985541120</td>\n      <td>1697126574</td>\n      <td>Solution. \"You'll need to be: 18+ and a UK res...</td>\n      <td>0</td>\n      <td>https://twitter.com/blazedstorm/status/1178446...</td>\n      <td>2019-09-29 23:07:46+00:00</td>\n      <td>Sun Sep 29 23:07:46 +0000 2019</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>mortgage</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1178446170722619393</td>\n      <td>1239955070</td>\n      <td>Kabaddi x3 UK Premier 1st show House Full Show...</td>\n      <td>0</td>\n      <td>https://twitter.com/habamoment/status/11784461...</td>\n      <td>2019-09-29 23:07:16+00:00</td>\n      <td>Sun Sep 29 23:07:16 +0000 2019</td>\n      <td>0</td>\n      <td>@Peepal</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>https://www.facebook.com/habteam/posts/1106547...</td>\n      <td>mortgage</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    " print(tweet_df_raw.shape);tweet_df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  sector company_name\n",
       "0  Banks     Barclays\n",
       "1  Banks       Lloyds\n",
       "2  Banks         HSBC\n",
       "3  Banks    Citi Bank\n",
       "4  Banks    Santander"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sector</th>\n      <th>company_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Banks</td>\n      <td>Barclays</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Banks</td>\n      <td>Lloyds</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Banks</td>\n      <td>HSBC</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Banks</td>\n      <td>Citi Bank</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Banks</td>\n      <td>Santander</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    " # create df object of companies looking to analyze\n",
    "fin_firms = pd.read_csv('../../data/fin_firms.csv')\n",
    "tweet_df_raw['text'] = tweet_df_raw['text'].str.lower()\n",
    "tweet_df_raw['company']=''\n",
    "fin_firms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "barclays\n",
      "lloyds\n",
      "hsbc\n",
      "citi bank\n",
      "santander\n",
      "nationwide\n",
      "allianz\n",
      "western union\n",
      "starling\n",
      "axa\n",
      "monzo\n",
      "revolut\n",
      "qbe\n"
     ]
    }
   ],
   "source": [
    " # locate instances of the firms being mentioned in the raw Twitter data\n",
    "for comp in fin_firms['company_name'].unique():\n",
    "    print(comp.lower())\n",
    "    tweet_df_raw.loc[tweet_df_raw['text'].str.contains(comp.lower()),'company'] = comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                0  \\\n",
       "id                                            1178457108276289536   \n",
       "author_id                                                40080176   \n",
       "text            this normalisation of no deal is horrendous. p...   \n",
       "retweets                                                        0   \n",
       "permalink       https://twitter.com/KatarinaKeys/status/117845...   \n",
       "date                                    2019-09-29 23:50:43+00:00   \n",
       "formatted_date                     Sun Sep 29 23:50:43 +0000 2019   \n",
       "favorites                                                       0   \n",
       "mentions                                                      NaN   \n",
       "hashtags                                                      NaN   \n",
       "geo                                                           NaN   \n",
       "urls                                                          NaN   \n",
       "search_term                                              mortgage   \n",
       "company                                                             \n",
       "sector                                                        NaN   \n",
       "company_name                                                  NaN   \n",
       "\n",
       "                                                                1  \\\n",
       "id                                            1178455823242035201   \n",
       "author_id                                     1126071201481334787   \n",
       "text            jumbo mortgage program https:// conclud.com/ht...   \n",
       "retweets                                                        0   \n",
       "permalink       https://twitter.com/Conclud2/status/1178455823...   \n",
       "date                                    2019-09-29 23:45:37+00:00   \n",
       "formatted_date                     Sun Sep 29 23:45:37 +0000 2019   \n",
       "favorites                                                       0   \n",
       "mentions                                                      NaN   \n",
       "hashtags                                                      NaN   \n",
       "geo                                                           NaN   \n",
       "urls            https://conclud.com/https-www-madisonmortgageg...   \n",
       "search_term                                              mortgage   \n",
       "company                                                             \n",
       "sector                                                        NaN   \n",
       "company_name                                                  NaN   \n",
       "\n",
       "                                                                2  \\\n",
       "id                                            1178450126219685893   \n",
       "author_id                                      729387514914603009   \n",
       "text            if you have no work it's harder to feed your k...   \n",
       "retweets                                                        0   \n",
       "permalink       https://twitter.com/cjhenrygonzo/status/117845...   \n",
       "date                                    2019-09-29 23:22:59+00:00   \n",
       "formatted_date                     Sun Sep 29 23:22:59 +0000 2019   \n",
       "favorites                                                       0   \n",
       "mentions                                                      NaN   \n",
       "hashtags                                                      NaN   \n",
       "geo                                                           NaN   \n",
       "urls                                                          NaN   \n",
       "search_term                                              mortgage   \n",
       "company                                                             \n",
       "sector                                                        NaN   \n",
       "company_name                                                  NaN   \n",
       "\n",
       "                                                                3  \\\n",
       "id                                            1178446295985541120   \n",
       "author_id                                              1697126574   \n",
       "text            solution. \"you'll need to be: 18+ and a uk res...   \n",
       "retweets                                                        0   \n",
       "permalink       https://twitter.com/blazedstorm/status/1178446...   \n",
       "date                                    2019-09-29 23:07:46+00:00   \n",
       "formatted_date                     Sun Sep 29 23:07:46 +0000 2019   \n",
       "favorites                                                       2   \n",
       "mentions                                                      NaN   \n",
       "hashtags                                                      NaN   \n",
       "geo                                                           NaN   \n",
       "urls                                                          NaN   \n",
       "search_term                                              mortgage   \n",
       "company                                                             \n",
       "sector                                                        NaN   \n",
       "company_name                                                  NaN   \n",
       "\n",
       "                                                                4  \n",
       "id                                            1178446170722619393  \n",
       "author_id                                              1239955070  \n",
       "text            kabaddi x3 uk premier 1st show house full show...  \n",
       "retweets                                                        0  \n",
       "permalink       https://twitter.com/habamoment/status/11784461...  \n",
       "date                                    2019-09-29 23:07:16+00:00  \n",
       "formatted_date                     Sun Sep 29 23:07:16 +0000 2019  \n",
       "favorites                                                       0  \n",
       "mentions                                                  @Peepal  \n",
       "hashtags                                                      NaN  \n",
       "geo                                                           NaN  \n",
       "urls            https://www.facebook.com/habteam/posts/1106547...  \n",
       "search_term                                              mortgage  \n",
       "company                                                            \n",
       "sector                                                        NaN  \n",
       "company_name                                                  NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id</th>\n      <td>1178457108276289536</td>\n      <td>1178455823242035201</td>\n      <td>1178450126219685893</td>\n      <td>1178446295985541120</td>\n      <td>1178446170722619393</td>\n    </tr>\n    <tr>\n      <th>author_id</th>\n      <td>40080176</td>\n      <td>1126071201481334787</td>\n      <td>729387514914603009</td>\n      <td>1697126574</td>\n      <td>1239955070</td>\n    </tr>\n    <tr>\n      <th>text</th>\n      <td>this normalisation of no deal is horrendous. p...</td>\n      <td>jumbo mortgage program https:// conclud.com/ht...</td>\n      <td>if you have no work it's harder to feed your k...</td>\n      <td>solution. \"you'll need to be: 18+ and a uk res...</td>\n      <td>kabaddi x3 uk premier 1st show house full show...</td>\n    </tr>\n    <tr>\n      <th>retweets</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>permalink</th>\n      <td>https://twitter.com/KatarinaKeys/status/117845...</td>\n      <td>https://twitter.com/Conclud2/status/1178455823...</td>\n      <td>https://twitter.com/cjhenrygonzo/status/117845...</td>\n      <td>https://twitter.com/blazedstorm/status/1178446...</td>\n      <td>https://twitter.com/habamoment/status/11784461...</td>\n    </tr>\n    <tr>\n      <th>date</th>\n      <td>2019-09-29 23:50:43+00:00</td>\n      <td>2019-09-29 23:45:37+00:00</td>\n      <td>2019-09-29 23:22:59+00:00</td>\n      <td>2019-09-29 23:07:46+00:00</td>\n      <td>2019-09-29 23:07:16+00:00</td>\n    </tr>\n    <tr>\n      <th>formatted_date</th>\n      <td>Sun Sep 29 23:50:43 +0000 2019</td>\n      <td>Sun Sep 29 23:45:37 +0000 2019</td>\n      <td>Sun Sep 29 23:22:59 +0000 2019</td>\n      <td>Sun Sep 29 23:07:46 +0000 2019</td>\n      <td>Sun Sep 29 23:07:16 +0000 2019</td>\n    </tr>\n    <tr>\n      <th>favorites</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>mentions</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>@Peepal</td>\n    </tr>\n    <tr>\n      <th>hashtags</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>geo</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>urls</th>\n      <td>NaN</td>\n      <td>https://conclud.com/https-www-madisonmortgageg...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>https://www.facebook.com/habteam/posts/1106547...</td>\n    </tr>\n    <tr>\n      <th>search_term</th>\n      <td>mortgage</td>\n      <td>mortgage</td>\n      <td>mortgage</td>\n      <td>mortgage</td>\n      <td>mortgage</td>\n    </tr>\n    <tr>\n      <th>company</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>sector</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>company_name</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "tweet_df_filtered = tweet_df_raw.merge(fin_firms, how='left', left_on='company', right_on='company_name')\n",
    "tweet_df_filtered.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "search_term\n",
       "credit card           2226\n",
       "current account        254\n",
       "insurance            10814\n",
       "investment           15673\n",
       "money transfer          69\n",
       "mortgage              4704\n",
       "pension               9347\n",
       "peronal loan            20\n",
       "savings account        182\n",
       "tax advice             146\n",
       "wealth management      269\n",
       "Name: id, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    " # search terms that were used most commonly with the associated firms\n",
    " tweet_df_filtered.groupby('search_term')['id'].count()"
   ]
  },
  {
   "source": [
    "## Topic Extraction: LDA Model\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra values\n",
    "# removing everthing that is not a regular expression\n",
    "is_url = re.compile(r'http[s]?:// (?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', re.VERBOSE | re.IGNORECASE)\n",
    "is_rt_username = re.compile(r'^RT+[\\s]+(@[\\w_]+:)',re.VERBOSE | re.IGNORECASE) #r'^RT+[\\s]+(@[\\w_]+:)'\n",
    "# removing tags\n",
    "is_entity = re.compile(r'@[\\w_]+', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "# print topics\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    \"\"\"\n",
    "    Prints the topics of the twitter data\n",
    "    \"\"\"\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])) \n",
    "\n",
    "# show top n keywords for each of the topics\n",
    "def show_topics(vectorizer, lda_model, n_words=20):\n",
    "    \"\"\"\n",
    "    Show the topcs and the most common keywords\n",
    "    \"\"\"\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "\n",
    "    return topic_keywords\n",
    "     \n",
    "        \n",
    "def clean_tweet(row):\n",
    "    \"\"\"\n",
    "    Clean the tweets of urls, usernames, and excess words\n",
    "    \"\"\"\n",
    "    row = is_url.sub(\"\",row)\n",
    "    row = is_rt_username.sub(\"\",row)\n",
    "    row = is_entity.sub(\"\",row)\n",
    "\n",
    "    return row\n",
    "\n",
    "def tokenize_only(text):\n",
    "    \"\"\"\n",
    "    Toeknize the tweets (Sentence -> words) and filter any numerical tokens\n",
    "    \"\"\"\n",
    "    # tokenize by sentence,then word\n",
    "    tokens = [word.lower() for sent in tok.sent_tokenize(text) for word in tok.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    id            author_id  \\\n",
       "0  1178457108276289536             40080176   \n",
       "1  1178455823242035201  1126071201481334787   \n",
       "2  1178450126219685893   729387514914603009   \n",
       "3  1178446295985541120           1697126574   \n",
       "4  1178446170722619393           1239955070   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  this normalisation of no deal is horrendous. p...         0   \n",
       "1  jumbo mortgage program https:// conclud.com/ht...         0   \n",
       "2  if you have no work it's harder to feed your k...         0   \n",
       "3  solution. \"you'll need to be: 18+ and a uk res...         0   \n",
       "4  kabaddi x3 uk premier 1st show house full show...         0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/KatarinaKeys/status/117845...   \n",
       "1  https://twitter.com/Conclud2/status/1178455823...   \n",
       "2  https://twitter.com/cjhenrygonzo/status/117845...   \n",
       "3  https://twitter.com/blazedstorm/status/1178446...   \n",
       "4  https://twitter.com/habamoment/status/11784461...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-29 23:50:43+00:00  Sun Sep 29 23:50:43 +0000 2019          0   \n",
       "1  2019-09-29 23:45:37+00:00  Sun Sep 29 23:45:37 +0000 2019          0   \n",
       "2  2019-09-29 23:22:59+00:00  Sun Sep 29 23:22:59 +0000 2019          0   \n",
       "3  2019-09-29 23:07:46+00:00  Sun Sep 29 23:07:46 +0000 2019          2   \n",
       "4  2019-09-29 23:07:16+00:00  Sun Sep 29 23:07:16 +0000 2019          0   \n",
       "\n",
       "  mentions hashtags  geo                                               urls  \\\n",
       "0      NaN      NaN  NaN                                                NaN   \n",
       "1      NaN      NaN  NaN  https://conclud.com/https-www-madisonmortgageg...   \n",
       "2      NaN      NaN  NaN                                                NaN   \n",
       "3      NaN      NaN  NaN                                                NaN   \n",
       "4  @Peepal      NaN  NaN  https://www.facebook.com/habteam/posts/1106547...   \n",
       "\n",
       "  search_term company sector company_name  \\\n",
       "0    mortgage            NaN          NaN   \n",
       "1    mortgage            NaN          NaN   \n",
       "2    mortgage            NaN          NaN   \n",
       "3    mortgage            NaN          NaN   \n",
       "4    mortgage            NaN          NaN   \n",
       "\n",
       "                                          text_clean  \n",
       "0  this normalisation of no deal is horrendous pe...  \n",
       "1  jumbo mortgage program https concludcomhttpsww...  \n",
       "2  if you have no work its harder to feed your ki...  \n",
       "3  solution youll need to be 18 and a uk resident...  \n",
       "4  kabaddi x3 uk premier 1st show house full show...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>author_id</th>\n      <th>text</th>\n      <th>retweets</th>\n      <th>permalink</th>\n      <th>date</th>\n      <th>formatted_date</th>\n      <th>favorites</th>\n      <th>mentions</th>\n      <th>hashtags</th>\n      <th>geo</th>\n      <th>urls</th>\n      <th>search_term</th>\n      <th>company</th>\n      <th>sector</th>\n      <th>company_name</th>\n      <th>text_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1178457108276289536</td>\n      <td>40080176</td>\n      <td>this normalisation of no deal is horrendous. p...</td>\n      <td>0</td>\n      <td>https://twitter.com/KatarinaKeys/status/117845...</td>\n      <td>2019-09-29 23:50:43+00:00</td>\n      <td>Sun Sep 29 23:50:43 +0000 2019</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>mortgage</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>this normalisation of no deal is horrendous pe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1178455823242035201</td>\n      <td>1126071201481334787</td>\n      <td>jumbo mortgage program https:// conclud.com/ht...</td>\n      <td>0</td>\n      <td>https://twitter.com/Conclud2/status/1178455823...</td>\n      <td>2019-09-29 23:45:37+00:00</td>\n      <td>Sun Sep 29 23:45:37 +0000 2019</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>https://conclud.com/https-www-madisonmortgageg...</td>\n      <td>mortgage</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>jumbo mortgage program https concludcomhttpsww...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1178450126219685893</td>\n      <td>729387514914603009</td>\n      <td>if you have no work it's harder to feed your k...</td>\n      <td>0</td>\n      <td>https://twitter.com/cjhenrygonzo/status/117845...</td>\n      <td>2019-09-29 23:22:59+00:00</td>\n      <td>Sun Sep 29 23:22:59 +0000 2019</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>mortgage</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>if you have no work its harder to feed your ki...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1178446295985541120</td>\n      <td>1697126574</td>\n      <td>solution. \"you'll need to be: 18+ and a uk res...</td>\n      <td>0</td>\n      <td>https://twitter.com/blazedstorm/status/1178446...</td>\n      <td>2019-09-29 23:07:46+00:00</td>\n      <td>Sun Sep 29 23:07:46 +0000 2019</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>mortgage</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>solution youll need to be 18 and a uk resident...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1178446170722619393</td>\n      <td>1239955070</td>\n      <td>kabaddi x3 uk premier 1st show house full show...</td>\n      <td>0</td>\n      <td>https://twitter.com/habamoment/status/11784461...</td>\n      <td>2019-09-29 23:07:16+00:00</td>\n      <td>Sun Sep 29 23:07:16 +0000 2019</td>\n      <td>0</td>\n      <td>@Peepal</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>https://www.facebook.com/habteam/posts/1106547...</td>\n      <td>mortgage</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>kabaddi x3 uk premier 1st show house full show...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    " # remove urls and retweets and entities from the text\n",
    "tweet_df_filtered['text_clean'] = tweet_df_filtered['text'].apply(lambda row:clean_tweet(row))\n",
    "\n",
    "# remove punctuations\n",
    "RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "tweet_df_filtered['text_clean'] = tweet_df_filtered['text_clean'].str.replace(RE_PUNCTUATION, \"\")\n",
    "tweet_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stopwords (used to remove stopwords)\n",
    "stop_words = stopwords.words('english') #import stopwords from NLTK package\n",
    "stop_words_df = pd.read_csv(\"../../data/pre_process_twitter_stop_words.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "stop_words_df = stop_words_df.wordList.tolist() # convert pandas data frame to a list\n",
    "stop_words_df.append('http')\n",
    "stop_words_df.append('https')\n",
    "\n",
    "# add in search terms as topic extraction is performed within each search topic, \n",
    "# we do not want the word or variation of the word captured as a topic word\n",
    "search_terms_revised = ['mortgages','wealthmanagement','pensions','money','transfer']\n",
    "stop_words_df.extend(search_terms)\n",
    "stop_words_df.extend(search_terms_revised)\n",
    "\n",
    "stop_list = stop_words + stop_words_df # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # store only unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter for lda\n",
    "number_topics = 5\n",
    "number_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seanm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "mortgage\n",
      "Topics found via LDA:\n",
      "     index   Word 0 Word 1  Word 2    Word 3  Word 4  topic_index\n",
      "0  Topic 0  adviser   jobs    pass   trainee   cemap            0\n",
      "1  Topic 1    house    pay  people       get   years            1\n",
      "2  Topic 2      pay   need   least     years  people            2\n",
      "3  Topic 3      get    buy     one  property    time            3\n",
      "4  Topic 4      pay    get    rent    people    know            4\n",
      "current account\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index   Word 0  Word 1  Word 2   Word 3   Word 4  topic_index\n",
      "0  Topic 0       hi    help    bank   online  banking            0\n",
      "1  Topic 1  deficit    card  switch  balance       im            1\n",
      "2  Topic 2  service     set     new    china     make            2\n",
      "3  Topic 3     also  switch     one    using    hello            3\n",
      "4  Topic 4      get    card      uk    debit   branch            4\n",
      "savings account\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index    Word 0   Word 1 Word 2  Word 3    Word 4  topic_index\n",
      "0  Topic 0  interest      got   card      go       buy            0\n",
      "1  Topic 1      bank     like    use  paying       new            1\n",
      "2  Topic 2       put  current   best     old  separate            2\n",
      "3  Topic 3      bank  already    try    like     joint            3\n",
      "4  Topic 4        go      see   dont    time    really            4\n",
      "insurance\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index    Word 0     Word 1  Word 2   Word 3    Word 4  topic_index\n",
      "0  Topic 0      jobs  insurtech  london      get  business            0\n",
      "1  Topic 1       car        get    like  company       one            1\n",
      "2  Topic 2        us        new     car      one      good            2\n",
      "3  Topic 3       nhs         us  health    cover   private            3\n",
      "4  Topic 4  national        pay  health      tax      need            4\n",
      "credit card\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index Word 0   Word 1 Word 2   Word 3 Word 4  topic_index\n",
      "0  Topic 0    pay      get   good      new   bank            0\n",
      "1  Topic 1    one  account    get      pay    got            1\n",
      "2  Topic 2    get      pay   time      got    use            2\n",
      "3  Topic 3    get      pay    app     need  apple            3\n",
      "4  Topic 4    pay      use   paid  payment   bank            4\n",
      "pension\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index Word 0    Word 1 Word 2      Word 3       Word 4  topic_index\n",
      "0  Topic 0   work       pay   paid       years        state            0\n",
      "1  Topic 1    get       pay   work       state          nhs            1\n",
      "2  Topic 2    tax     state    get         pay       people            2\n",
      "3  Topic 3     eu       get   fund         age       people            3\n",
      "4  Topic 4   jobs  benefits   join  vulnerable  financially            4\n",
      "peronal loan\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index        Word 0    Word 1      Word 2    Word 3   Word 4  topic_index\n",
      "0  Topic 0          help      taxi      driver  customer     paid            0\n",
      "1  Topic 1      business    liable  personally     david     made            1\n",
      "2  Topic 2  selfemployed      know        back       get  account            2\n",
      "3  Topic 3         spend  purchase      credit     think    first            3\n",
      "4  Topic 4           via    market     fintech   purpose    years            4\n",
      "money transfer\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index         Word 0         Word 1      Word 2    Word 3         Word 4  \\\n",
      "0  Topic 0  moneytransfer           dexa     fintech        rt       dexacoin   \n",
      "1  Topic 1  moneytransfer          great     fintech       day          rates   \n",
      "2  Topic 2         people  moneytransfer  remittance       way        contact   \n",
      "3  Topic 3  moneytransfer             us      travel  exchange  international   \n",
      "4  Topic 4       services            use        haha   balance           sent   \n",
      "\n",
      "   topic_index  \n",
      "0            0  \n",
      "1            1  \n",
      "2            2  \n",
      "3            3  \n",
      "4            4  \n",
      "tax advice\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index     Word 0    Word 1       Word 2     Word 3 Word 4  topic_index\n",
      "0  Topic 0     people       get        years      older    age            0\n",
      "1  Topic 1  avoidance  business  inheritance  taxadvice   hmrc            1\n",
      "2  Topic 2    council      hmrc         like       need   help            2\n",
      "3  Topic 3         us      care     business     income   need            3\n",
      "4  Topic 4         uk    brexit     business       free   like            4\n",
      "investment\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index       Word 0       Word 1  Word 2   Word 3    Word 4  topic_index\n",
      "0  Topic 0  investments       people  brexit     need       get            0\n",
      "1  Topic 1           uk         stop    time  killing     spies            1\n",
      "2  Topic 2  investments          new    good     like  property            2\n",
      "3  Topic 3  investments         need   great      new      good            3\n",
      "4  Topic 4           uk  investments     new       us       get            4\n",
      "wealth management\n",
      "C:\\Users\\seanm\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'account', 'across', 'advice', 'ai', 'almost', 'also', 'among', 'ca', 'card', 'credit', 'current', 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', 'however', 'least', 'let', 'like', 'likely', 'loan', 'management', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'personal', 'rather', 'said', 'savings', 'say', 'says', 'sha', 'since', 'tax', 'tis', 'twas', 'us', 'wants', 'wealth', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "Topics found via LDA:\n",
      "     index  Word 0      Word 1     Word 2          Word 3    Word 4  \\\n",
      "0  Topic 0  wealth  management       says         company       day   \n",
      "1  Topic 1  wealth  management         uk        planning       tax   \n",
      "2  Topic 2  wealth  management  financial      wealthtech  business   \n",
      "3  Topic 3  wealth  management   business      wealthtech       one   \n",
      "4  Topic 4  wealth  management    wealthy  wealthbuilding    london   \n",
      "\n",
      "   topic_index  \n",
      "0            0  \n",
      "1            1  \n",
      "2            2  \n",
      "3            3  \n",
      "4            4  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "tweets_all_topics = pd.DataFrame()\n",
    "# term frequency modeling\n",
    "for terms in tweet_df_comp['search_term'].unique():\n",
    "    print(terms)\n",
    "    tweets_search_topics  = tweet_df_filtered[tweet_df_filtered['search_term']==terms].reset_index(drop=True)\n",
    "    corpus = tweets_search_topics['text_clean'].tolist()\n",
    "    # print(corpus)\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only) # use tf (raw term      count) features for LDA.\n",
    "    tf = tf_vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # create and fit the LDA model\n",
    "    model = LDA(n_components=number_topics, n_jobs=-1)\n",
    "    id_topic = model.fit(tf)\n",
    "    # print the topics found by the LDA model\n",
    "    print(\"Topics found via LDA:\")\n",
    "    topic_keywords = show_topics(vectorizer=tf_vectorizer, lda_model=model, n_words=number_words)        \n",
    "    # topic - Keywords Dataframe\n",
    "    df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "    df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "    df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "    df_topic_keywords = df_topic_keywords.reset_index()\n",
    "    df_topic_keywords['topic_index'] = df_topic_keywords['index'].str.split(' ', n = 1, expand = True)[[1]].astype('int')\n",
    "    print(df_topic_keywords)\n",
    "    \n",
    "    ############ get the dominat topic for each document in a data frame ###############\n",
    "    # create document  Topic Matrix\n",
    "    lda_output = model.transform(tf)\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(model.n_components)]\n",
    "    # index names\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(corpus))]\n",
    "    \n",
    "    # make pandas dataframe\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "    # get dominant topic for each document\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic   \n",
    "    df_document_topic = df_document_topic.reset_index()\n",
    "        \n",
    "    # combine all the search terms into one data frame\n",
    "    tweets_topics = tweets_search_topics.merge(df_document_topic, left_index=True, right_index=True, how='left')\n",
    "    tweets_topics_words = tweets_topics.merge(df_topic_keywords, how='left', left_on='dominant_topic', right_on='topic_index')\n",
    "    tweets_all_topics = tweets_all_topics.append(tweets_topics_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(43704, 31)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    id            author_id  \\\n",
       "0  1178457108276289536             40080176   \n",
       "1  1178455823242035201  1126071201481334787   \n",
       "2  1178450126219685893   729387514914603009   \n",
       "3  1178446295985541120           1697126574   \n",
       "4  1178446170722619393           1239955070   \n",
       "\n",
       "                                                text  retweets  \\\n",
       "0  this normalisation of no deal is horrendous. p...         0   \n",
       "1  jumbo mortgage program https:// conclud.com/ht...         0   \n",
       "2  if you have no work it's harder to feed your k...         0   \n",
       "3  solution. \"you'll need to be: 18+ and a uk res...         0   \n",
       "4  kabaddi x3 uk premier 1st show house full show...         0   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/KatarinaKeys/status/117845...   \n",
       "1  https://twitter.com/Conclud2/status/1178455823...   \n",
       "2  https://twitter.com/cjhenrygonzo/status/117845...   \n",
       "3  https://twitter.com/blazedstorm/status/1178446...   \n",
       "4  https://twitter.com/habamoment/status/11784461...   \n",
       "\n",
       "                        date                  formatted_date  favorites  \\\n",
       "0  2019-09-29 23:50:43+00:00  Sun Sep 29 23:50:43 +0000 2019          0   \n",
       "1  2019-09-29 23:45:37+00:00  Sun Sep 29 23:45:37 +0000 2019          0   \n",
       "2  2019-09-29 23:22:59+00:00  Sun Sep 29 23:22:59 +0000 2019          0   \n",
       "3  2019-09-29 23:07:46+00:00  Sun Sep 29 23:07:46 +0000 2019          2   \n",
       "4  2019-09-29 23:07:16+00:00  Sun Sep 29 23:07:16 +0000 2019          0   \n",
       "\n",
       "  mentions hashtags  ...  Topic3 Topic4 dominant_topic  index_y Word 0 Word 1  \\\n",
       "0      NaN      NaN  ...    0.01   0.01              1  Topic 1  house    pay   \n",
       "1      NaN      NaN  ...    0.04   0.04              2  Topic 2    pay   need   \n",
       "2      NaN      NaN  ...    0.01   0.01              1  Topic 1  house    pay   \n",
       "3      NaN      NaN  ...    0.01   0.01              2  Topic 2    pay   need   \n",
       "4  @Peepal      NaN  ...    0.02   0.02              1  Topic 1  house    pay   \n",
       "\n",
       "   Word 2 Word 3  Word 4  topic_index  \n",
       "0  people    get   years            1  \n",
       "1   least  years  people            2  \n",
       "2  people    get   years            1  \n",
       "3   least  years  people            2  \n",
       "4  people    get   years            1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>author_id</th>\n      <th>text</th>\n      <th>retweets</th>\n      <th>permalink</th>\n      <th>date</th>\n      <th>formatted_date</th>\n      <th>favorites</th>\n      <th>mentions</th>\n      <th>hashtags</th>\n      <th>...</th>\n      <th>Topic3</th>\n      <th>Topic4</th>\n      <th>dominant_topic</th>\n      <th>index_y</th>\n      <th>Word 0</th>\n      <th>Word 1</th>\n      <th>Word 2</th>\n      <th>Word 3</th>\n      <th>Word 4</th>\n      <th>topic_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1178457108276289536</td>\n      <td>40080176</td>\n      <td>this normalisation of no deal is horrendous. p...</td>\n      <td>0</td>\n      <td>https://twitter.com/KatarinaKeys/status/117845...</td>\n      <td>2019-09-29 23:50:43+00:00</td>\n      <td>Sun Sep 29 23:50:43 +0000 2019</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>Topic 1</td>\n      <td>house</td>\n      <td>pay</td>\n      <td>people</td>\n      <td>get</td>\n      <td>years</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1178455823242035201</td>\n      <td>1126071201481334787</td>\n      <td>jumbo mortgage program https:// conclud.com/ht...</td>\n      <td>0</td>\n      <td>https://twitter.com/Conclud2/status/1178455823...</td>\n      <td>2019-09-29 23:45:37+00:00</td>\n      <td>Sun Sep 29 23:45:37 +0000 2019</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.04</td>\n      <td>0.04</td>\n      <td>2</td>\n      <td>Topic 2</td>\n      <td>pay</td>\n      <td>need</td>\n      <td>least</td>\n      <td>years</td>\n      <td>people</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1178450126219685893</td>\n      <td>729387514914603009</td>\n      <td>if you have no work it's harder to feed your k...</td>\n      <td>0</td>\n      <td>https://twitter.com/cjhenrygonzo/status/117845...</td>\n      <td>2019-09-29 23:22:59+00:00</td>\n      <td>Sun Sep 29 23:22:59 +0000 2019</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>1</td>\n      <td>Topic 1</td>\n      <td>house</td>\n      <td>pay</td>\n      <td>people</td>\n      <td>get</td>\n      <td>years</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1178446295985541120</td>\n      <td>1697126574</td>\n      <td>solution. \"you'll need to be: 18+ and a uk res...</td>\n      <td>0</td>\n      <td>https://twitter.com/blazedstorm/status/1178446...</td>\n      <td>2019-09-29 23:07:46+00:00</td>\n      <td>Sun Sep 29 23:07:46 +0000 2019</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>Topic 2</td>\n      <td>pay</td>\n      <td>need</td>\n      <td>least</td>\n      <td>years</td>\n      <td>people</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1178446170722619393</td>\n      <td>1239955070</td>\n      <td>kabaddi x3 uk premier 1st show house full show...</td>\n      <td>0</td>\n      <td>https://twitter.com/habamoment/status/11784461...</td>\n      <td>2019-09-29 23:07:16+00:00</td>\n      <td>Sun Sep 29 23:07:16 +0000 2019</td>\n      <td>0</td>\n      <td>@Peepal</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>1</td>\n      <td>Topic 1</td>\n      <td>house</td>\n      <td>pay</td>\n      <td>people</td>\n      <td>get</td>\n      <td>years</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "tweets_all_topics = tweets_all_topics.reset_index(drop=True)\n",
    "print(tweets_all_topics.shape)\n",
    "tweets_all_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all_topics.to_csv('../../data/tweets_all_topics.csv', index=False)"
   ]
  },
  {
   "source": [
    "## Sentiment Analysis with Deep Learning\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "I have trained the model usign the movie review data. The details of the training of the model can be found here: https://towardsdatascience.com/sentiment-analysis-for-text-with-deep-learning-2f0a0c6472b5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting keras\n  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\nRequirement already satisfied: scipy>=0.14 in c:\\users\\seanm\\anaconda3\\lib\\site-packages (from keras) (1.4.1)\nRequirement already satisfied: pyyaml in c:\\users\\seanm\\anaconda3\\lib\\site-packages (from keras) (5.3)\nRequirement already satisfied: h5py in c:\\users\\seanm\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\nRequirement already satisfied: numpy>=1.9.1 in c:\\users\\seanm\\anaconda3\\lib\\site-packages (from keras) (1.18.5)\nRequirement already satisfied: six in c:\\users\\seanm\\anaconda3\\lib\\site-packages (from h5py->keras) (1.14.0)\nInstalling collected packages: keras\nSuccessfully installed keras-2.4.3\nNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-4f6997ccbe9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mweight_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../../data/model/dl_model.hdf5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprd_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprd_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mword_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../../data/model/word_idx.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    182\u001b[0m     if (h5py is not None and (\n\u001b[0;32m    183\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m--> 184\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[1;32m--> 178\u001b[1;33m                                                custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;31m# set weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    107\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[1;32m--> 362\u001b[1;33m         config, module_objects, custom_objects, printable_module_name)\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[1;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[0mcls_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m   \u001b[0mdeserialized_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'__passive_serialization__'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m       deserialized_objects[key] = deserialize_keras_object(\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# read in the weight of the trained model\n",
    "weight_path = '../../data/model/dl_model.hdf5'\n",
    "\n",
    "prd_model = load_model(weight_path)\n",
    "prd_model.summary()\n",
    "word_idx = json.load(open(\"../../data/model/word_idx.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_DL(prd_model, text_data, word_idx):\n",
    "\n",
    "    # data = \"Pass the salt\"\n",
    "\n",
    "    live_list = []\n",
    "    batchSize = len(text_data)\n",
    "    live_list_np = np.zeros((56,batchSize))\n",
    "    for index, row in text_data.iterrows():\n",
    "        # print (index)\n",
    "        text_data_sample = text_data['text'][index]\n",
    "        # split the sentence into its words and remove any punctuations.\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text_data_list = tokenizer.tokenize(text_data_sample)\n",
    "\n",
    "        # text_data_list = text_data_sample.split()\n",
    "\n",
    "\n",
    "        labels = np.array(['1','2','3','4','5','6','7','8','9','10'], dtype = \"int\")\n",
    "        # word_idx['I']\n",
    "        # get index for the live stage\n",
    "        data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in text_data_list])\n",
    "        data_index_np = np.array(data_index)\n",
    "\n",
    "        # padded with zeros of length 56 i.e maximum length\n",
    "        padded_array = np.zeros(56)\n",
    "        padded_array[:data_index_np.shape[0]] = data_index_np[:56]\n",
    "        data_index_np_pad = padded_array.astype(int)\n",
    "\n",
    "\n",
    "        live_list.append(data_index_np_pad)\n",
    "\n",
    "    live_list_np = np.asarray(live_list)\n",
    "    score = prd_model.predict(live_list_np, batch_size=batchSize, verbose=0)\n",
    "    single_score = np.round(np.dot(score, labels)/10,decimals=2)\n",
    "\n",
    "    score_all  = []\n",
    "    for each_score in score:\n",
    "\n",
    "        top_3_index = np.argsort(each_score)[-3:]\n",
    "        top_3_scores = each_score[top_3_index]\n",
    "        top_3_weights = top_3_scores/np.sum(top_3_scores)\n",
    "        single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n",
    "        score_all.append(single_score_dot)\n",
    "\n",
    "    text_data['Sentiment_Score'] = pd.DataFrame(score_all)\n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'prd_model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-432d72b1f05c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext_data\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtweets_all_topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# deep Learning sentiment scoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentiment_DL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprd_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'prd_model' is not defined"
     ]
    }
   ],
   "source": [
    "text_data =  tweets_all_topics\n",
    "# deep Learning sentiment scoring\n",
    "text_out = get_sentiment_DL(prd_model, text_data, word_idx)"
   ]
  },
  {
   "source": [
    "### Example negative tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'text_out' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bcab95640414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Sentiment_Score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Sentiment_Score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text_out' is not defined"
     ]
    }
   ],
   "source": [
    "text_out.sort_values(by='Sentiment_Score')[['text','Sentiment_Score']].head().T "
   ]
  },
  {
   "source": [
    "### Example positive tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'text_out' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6605dad01950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Sentiment_Score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Sentiment_Score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text_out' is not defined"
     ]
    }
   ],
   "source": [
    "text_out.sort_values(by='Sentiment_Score', ascending=False)[['text','Sentiment_Score']].head().T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'text_out' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bc5a238800b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save the output files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../processed_data/tweets_topics_sentiment.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text_out' is not defined"
     ]
    }
   ],
   "source": [
    "# save the output files\n",
    "text_out.to_csv('../processed_data/tweets_topics_sentiment.csv', index=False)"
   ]
  },
  {
   "source": [
    "## Named Entity Recognition\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The below section is implementing a stanford 3 class NER tagger. The model is trained based on on supervised Conditional Random Field (CRF) model. Additional information on the model is available at https://nlp.stanford.edu/software/CRF-NER.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NER(text_data):\n",
    "    # /Users/prajwalshreyas/Desktop/Singularity/dockerApps/ner-algo/stanford-ner-2015-01-30\n",
    "    stanford_classifier = '../models/ner/english.all.3class.distsim.crf.ser.gz'\n",
    "    stanford_ner_path = '../models/ner/stanford-ner.jar'\n",
    "\n",
    "    # try:\n",
    "        # Creating Tagger Object\n",
    "    st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "    # except Exception as e:\n",
    "    #       print (e)\n",
    "\n",
    "    # get keyword for the input data frame\n",
    "    # keyword = tweetDataFrame.keyword.unique()\n",
    "    # Subset column containing tweet text and convert to list\n",
    "    # next insert a placeholder ' 12345678 ' to signify end of individual tweets\n",
    "\n",
    "    # text_data = pd.read_json('/Users/prajwalshreyas/Desktop/Singularity/dockerApps/sentiment-algo/ app-sentiment-algo/sample_text.json')\n",
    "    print ('start get_NER')\n",
    "    text_out = text_data.copy()\n",
    "    doc = [ docs + ' 12345678 ' for docs in list(text_data['text'])]\n",
    "    # ------------------------- Stanford Named Entity Recognition\n",
    "    tokens = nltk.word_tokenize(str(doc))\n",
    "    entities = st.tag(tokens) # actual tagging takes place using Stanford NER algorithm\n",
    "\n",
    "\n",
    "    entities = [list(elem) for elem in entities] # Convert list of tuples to list of list\n",
    "    print ('tag complete')\n",
    "    for idx,element in enumerate(entities):\n",
    "        try:\n",
    "            if entities[idx][0] == '12345678':\n",
    "                entities[idx][1] = \"DOC_NUMBER\"  #  Modify data by adding the tag \"Doc_Number\"\n",
    "            # elif entities[idx][0].lower() == keyword:\n",
    "            # entities[idx][1] = \"KEYWORD\"\n",
    "            # combine First and Last name into a single word\n",
    "            elif entities[idx][1] == \"PERSON\" and entities[idx + 1][1] == \"PERSON\":\n",
    "                entities[idx + 1][0] = entities[idx][0] + '-' + entities[idx+1][0]\n",
    "                entities[idx][1] = 'Combined'\n",
    "            # combine consecutive Organization names\n",
    "            elif entities[idx][1] == 'ORGANIZATION' and entities[idx + 1][1] == 'ORGANIZATION':\n",
    "                entities[idx + 1][0] = entities[idx][0] + '-' + entities[idx+1][0]\n",
    "                entities[idx][1] = 'Combined'\n",
    "        except IndexError:\n",
    "            break\n",
    "    print ('enumerate complete')\n",
    "    # filter list of list for the words we are interested in\n",
    "    filter_list = ['DOC_NUMBER','PERSON','LOCATION','ORGANIZATION']\n",
    "    entityWordList = [element for element in entities if any(i in element for i in filter_list)]\n",
    "\n",
    "    entityString = ' '.join(str(word) for insideList in entityWordList for word in insideList) \n",
    "    # convert list to string and concatenate it\n",
    "    entitySubString = entityString.split(\"DOC_NUMBER\") # split the string using the separator 'TWEET_NUMBER'\n",
    "    del entitySubString[-1] # delete the extra blank row created in the previous step\n",
    "\n",
    "    # store the classified NERs in the main tweet data frame\n",
    "    for idx,docNER in enumerate(entitySubString):\n",
    "        docNER = docNER.strip().split() # split the string into word list\n",
    "        # filter for words tagged as Organization and store it in data frame\n",
    "        text_out.loc[idx,'Organization'] =  ','.join([docNER[i-1]  for i,x in enumerate(docNER) if x\n",
    "        == 'ORGANIZATION'])\n",
    "        # filter for words tagged as LOCATION and store it in data frame\n",
    "        text_out.loc[idx,'Place'] = ','.join([docNER[i-1] for i,x in enumerate(docNER) if x ==\n",
    "        'LOCATION'])\n",
    "        # filter for words tagged as PERSON and store it in data frame\n",
    "        text_out.loc[idx,'Person'] = ','.join([docNER[i-1]  for i,x in enumerate(docNER) if x ==   \n",
    "        'PERSON'])   \n",
    "\n",
    "    print ('process complete')\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_NER' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e5547e8ad3b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_ner_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_NER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_NER' is not defined"
     ]
    }
   ],
   "source": [
    "text_ner_out = get_NER(text_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'text_ner_out' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c355d0107e44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#the outputs of the ner tagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext_ner_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ner_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Place'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext_ner_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Organization'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m|\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ner_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Person'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Organization'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Place'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Person'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text_ner_out' is not defined"
     ]
    }
   ],
   "source": [
    "# the outputs of the ner tagger\n",
    "text_ner_out.loc[(text_ner_out['Place'] != '') | (text_ner_out['Organization'] != '')|(text_ner_out['Person'] != '')][['text','Organization','Place','Person']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'text_ner_out' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4e5ddeadaf93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_ner_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../processed_data/tweets_topics_sentiment_ner.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text_ner_out' is not defined"
     ]
    }
   ],
   "source": [
    "text_ner_out.to_csv('../processed_data/tweets_topics_sentiment_ner.csv', index=False)"
   ]
  }
 ]
}