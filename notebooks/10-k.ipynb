{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bit99dc583a27f74139877115c1fd523946",
   "display_name": "Python 3.8.3 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "----\n",
    "# Analyzing SEC Filings textual data using NLP\n",
    "----\n",
    "----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Table Of Contents:\n",
    "\n",
    "1. Download Data\n",
    "2. Preprocessing / Cleaning\n",
    "3. Transformation (lemmatization) / Dependency mapping\n",
    "4. Model Training\n",
    "5. Model Evaluation\n",
    "----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1 Import Packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from secedgar.filings import Filing, FilingType\n",
    "import sys\n",
    "import os\n",
    "import util\n",
    "import nltk \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import alphalens as al\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords \n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "source": [
    "## 1.2 Download Corpora (data required for textual analysis)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\seanm\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\seanm\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# download text data that will be used for word netting (lemmatizing) and for removing (filler) stop words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "source": [
    "## 1.3 Download SEC Filing Data\n",
    "\n",
    "Here we will download a series of 10-k doucments (annual financial filings) that we can use to for our NLP analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_lookup = {\n",
    "    'AMZN': '0001018724',\n",
    "    'BMY': '0000014272',   \n",
    "    'CNP': '0001130310',\n",
    "    'CVX': '0000093410',\n",
    "    'FL': '0000850209',\n",
    "    'FRT': '0000034903',\n",
    "    'HON': '0000773840'}\n",
    "\n",
    "additional_cik = {\n",
    "    'AEP': '0000004904',\n",
    "    'AXP': '0000004962',\n",
    "    'BA': '0000012927', \n",
    "    'BK': '0001390777',\n",
    "    'CAT': '0000018230',\n",
    "    'DE': '0000315189',\n",
    "    'DIS': '0001001039',\n",
    "    'DTE': '0000936340',\n",
    "    'ED': '0001047862',\n",
    "    'EMR': '0000032604',\n",
    "    'ETN': '0001551182',\n",
    "    'GE': '0000040545',\n",
    "    'IBM': '0000051143',\n",
    "    'IP': '0000051434',\n",
    "    'JNJ': '0000200406',\n",
    "    'KO': '0000021344',\n",
    "    'LLY': '0000059478',\n",
    "    'MCD': '0000063908',\n",
    "    'MO': '0000764180',\n",
    "    'MRK': '0000310158',\n",
    "    'MRO': '0000101778',\n",
    "    'PCG': '0001004980',\n",
    "    'PEP': '0000077476',\n",
    "    'PFE': '0000078003',\n",
    "    'PG': '0000080424',\n",
    "    'PNR': '0000077360',\n",
    "    'SYY': '0000096021',\n",
    "    'TXN': '0000097476',\n",
    "    'UTX': '0000101829',\n",
    "    'WFC': '0000072971',\n",
    "    'WMT': '0000104169',\n",
    "    'WY': '0000106535',\n",
    "    'XOM': '0000034088'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_api = util.SecHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_sec_data(cik, doc_type, start=0, count=60, date='2018-01-01'):\n",
    "        \"\"\"\n",
    "        Function downloads SEC file data\n",
    "\n",
    "        :param cik:\n",
    "        :param doc_type:\n",
    "        :param start:\n",
    "        :param count:\n",
    "        :param data: Newest Pricing data date\n",
    "        :return entries:\n",
    "        \"\"\"\n",
    "        new_price_data = pd.to_datetime(date)\n",
    "        url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "            '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom'.format(cik, doc_type, start, count)\n",
    "        sec_data = sec_api.get(url)\n",
    "\n",
    "        feed = BeautifulSoup(sec_data.encode('ascii'), 'xml').feed\n",
    "        entries = [\n",
    "            (\n",
    "                entry.content.find('filing-href').getText(),\n",
    "                entry.content.find('filing-type').getText(),\n",
    "                entry.content.find('filing-date').getText())\n",
    "\n",
    "            for entry in feed .find_all('entry', recursive=False)\n",
    "            if pd.to_datetime(entry.content.find('filing-date').getText()) <= new_price_data]\n",
    "\n",
    "        return entries\n"
   ]
  },
  {
   "source": [
    "## 1.4 Pull a ticker from the list and analyze its documents\n",
    "\n",
    "Here we are going to use AMZN as an example and print the 10-k file information from various years prior to 2018."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('https://www.sec.gov/Archives/edgar/data/1018724/000101872417000011/0001018724-17-000011-index.htm',\n  '10-K',\n  '2017-02-10'),\n ('https://www.sec.gov/Archives/edgar/data/1018724/000101872416000172/0001018724-16-000172-index.htm',\n  '10-K',\n  '2016-01-29'),\n ('https://www.sec.gov/Archives/edgar/data/1018724/000101872415000006/0001018724-15-000006-index.htm',\n  '10-K',\n  '2015-01-30'),\n ('https://www.sec.gov/Archives/edgar/data/1018724/000101872414000006/0001018724-14-000006-index.htm',\n  '10-K',\n  '2014-01-31'),\n ('https://www.sec.gov/Archives/edgar/data/1018724/000119312513028520/0001193125-13-028520-index.htm',\n  '10-K',\n  '2013-01-30')]\n"
     ]
    }
   ],
   "source": [
    "example_ticker = 'AMZN'\n",
    "sec_data = {}\n",
    "\n",
    "for ticker, cik in cik_lookup.items():\n",
    "    sec_data[ticker] = get_sec_data(cik, '10-k')\n",
    "\n",
    "pprint.pprint(sec_data[example_ticker][:5])"
   ]
  },
  {
   "source": [
    "## 1.5 Download using the data using the acquired list of URLs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_filing_by_ticker = {}\n",
    "\n",
    "# for key, val in dict\n",
    "for ticker, data in sec_data.items():\n",
    "    raw_filing_by_ticker[ticker] = {}\n",
    "    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Filings'.format(ticker), unit='filing'):\n",
    "        file_url = index_url.replace('-index.htm', '.txt').replace('.txt1', '.txt')\n",
    "\n",
    "        raw_filing_by_ticker[ticker][file_date] = sec_api.get(file_url)\n",
    "\n",
    "print('Example Download:\\n\\n{}...'.format(next(iter(raw_filing_by_ticker[example_ticker].values()))[:1000]))"
   ]
  },
  {
   "source": [
    "## 1.6 Get Document Types"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_ks_ticker = {}\n",
    "\n",
    "for ticker, filing_documents in raw_filing_by_ticker.items():\n",
    "    ten_ks_ticker[ticker] = []\n",
    "    for file_date, documents in filing_documents.items():\n",
    "        for document in documents:\n",
    "            if get_document_type(document) == '10-k':\n",
    "                ten_ks_ticker[ticker].append({\n",
    "                    'cik': cik_lookup[ticker],\n",
    "                    'file': document,\n",
    "                    'file_date': file_date\n",
    "                })\n",
    "util.print_ten_k_data(ten_ks_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
   ]
  },
  {
   "source": [
    "----\n",
    "# Preprocessing\n",
    "----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 Clean Data\n",
    "Start by removing the messy html and making all the text lowercase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker, ten_ks in ten_ks_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_clean'] = util.clean_text(ten_k['file'])\n",
    "\n",
    "util.print_ten_k_data(ten_ks_ticker[example_ticker][:5], ['file_clean'])"
   ]
  },
  {
   "source": [
    "## 2.2 Lemmatize\n",
    "This is the process of distilling the verbs down and understanding the dependencies of various tokens in the sentence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pattern = re.compile('\\w+')\n",
    "\n",
    "for ticker, ten_ks in ten_ks_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_lemma'] =lemmatize_words(word_pattern.findall(ten_k['file_clean']))\n",
    "\n",
    "util.print_ten_k_data(ten_ks_ticker[example_ticker][:5], ['file_name'])\n"
   ]
  },
  {
   "source": [
    "## 2.3 Remove Stop Words\n",
    "\n",
    "Here we are removing words that are common in sentence structure and add noise to our dataset. Words such as 'and' and 'the' are stop words and should be removed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
    "\n",
    "for ticker, ten_ks in ten_ks_ticker.tiems():\n",
    "    for ten_k in tqdm(ten_ks, desc='Remove Stopwords for {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        # iterate through the documents in that we have lemmatized and remove stopwords\n",
    "        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]\n",
    "\n",
    "print('Stop Words Removed')\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "----\n",
    "# Analysis of 10-K\n",
    "----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.1 Loughranm McDonald Sentiment word list\n",
    "This will be used to determine the following sentiment for the filings:\n",
    "\n",
    "* Negative\n",
    "* Postive\n",
    "* Uncertainty\n",
    "* Litigous\n",
    "* Constraining\n",
    "* Superfluous\n",
    "* Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a list of the various sentiment types\n",
    "sentiments = ['negative', 'positive', 'uncertainty', 'litigious', 'constraining', 'interesting']\n",
    "# read in the sentiment word list\n",
    "sentiment_df = pd.read_csv(os.path.join('..', '..', 'data', 'project_5_loughran_mcdonald', 'loughran_mcdonald_master_dic_2016.csv'))\n",
    "# lowercase the columns\n",
    "sentiment_df.columns = [column.lower() for column in sentiment_df.columns] \n",
    "\n",
    "\n",
    "# remove unused data (we only want the columns specified in the list sentiments)\n",
    "sentiment_df = sentiment_df[sentiments + ['word']]\n",
    "sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)\n",
    "sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]\n",
    "\n",
    "# apply the same preprocessing that is applied to the words in the 10-K\n",
    "sentiment_df['word'] = lemmatize_words(sentiment_df['word'].str.lower())\n",
    "sentiments = sentiment_df.drop_duplicates('word')\n",
    "\n",
    "sentiment_df.head()\n"
   ]
  },
  {
   "source": [
    "## 3.2 Bag of Words\n",
    "\n",
    "With the sentiment words list that we just created we can generate a sentiment bag of words for our 10_k data set. Count the number of 'sentiment' words that we have in each doc (ignore the rest)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_bow_ten_ks = {}\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]\n",
    "\n",
    "    sentiment_bow_ten_ks[ticker] = {\n",
    "        sentiment: util.get_bag_of_words(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
    "        for sentiment in sentiments}\n",
    "\n",
    "util.print_ten_k_data([sentiment_bow_ten_ks[example_ticker]], sentiments)"
   ]
  },
  {
   "source": [
    "## 3.3 Jaccard Similarity\n",
    "\n",
    "Calculate the Jaccard Similarity of our our bag of words looking for similarities between each tick in time. (need to turn the bag of words into a boolean array for Jaccard Similarity calculation)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dates for the universe\n",
    "file_dates = {\n",
    "    ticker: [ten_k['file_date'] for ten_k in ten_ks]\n",
    "    for ticker, ten_ks in ten_ks_ticker.items()}\n",
    "\n",
    "jaccard_similarities = {\n",
    "    ticker: {\n",
    "        sentiment_name: get_jaccard_similarity(sentiment_values)\n",
    "        for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n",
    "    for ticker, ten_k_sentiments in sentiment_bow_ten_ks.items()}\n",
    "\n",
    "util.plot_similarities(\n",
    "    [jaccard_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "    file_dates[example_ticker][1:]\n",
    "    'Jaccard Similarities for {} Sentiment'.format(example_ticker),\n",
    "    sentiments)"
   ]
  },
  {
   "source": [
    "## 3.4 TFIDF\n",
    "\n",
    "Generate TFDIF from the 10-K documents using the sentiment word list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = {\n",
    "    ticker: {\n",
    "        sentiment_name: get_cosine_similarity(sentiment_values)\n",
    "        for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n",
    "    for ticker, ten_k_sentiments in sentiment_tfidf_ten_ks.items()}\n",
    "\n",
    "util.plot_similarities(\n",
    "    [cosine_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "    file_dates[example_ticker][1:],\n",
    "    'Cosine Similarities for {} Sentiment'.format(example_ticker),\n",
    "    sentiments)"
   ]
  },
  {
   "source": [
    "----\n",
    "# Evaluate Alpha Factors\n",
    "----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Analyze the returns of our cosine simalarities and Jaccard simalarities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.1 Price Data\n",
    "\n",
    "Yearly pricing to run the factor model against (10-Ks are annual filings)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing = pd.read_csv('../../data/pricing-data', parse_dates=['date'])\n",
    "prcing = prcing.pivot(index='date', columns='ticker', values='adj_close')\n",
    "\n",
    "pricing"
   ]
  },
  {
   "source": [
    "## 4.2 Dict to DataFrame\n",
    "\n",
    "Convert to DF since the alphalens library uses DataFrames."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = {}\n",
    "skipped_sentiments = []\n",
    "\n",
    "for senitment in sentiments:\n",
    "    cs_df = cosine_similarities_df[(cosine_similarities_df['sentiment'] == sentiment)]\n",
    "    cs_df = cs_df.pivot(index='date', columns='ticker', values='value')\n",
    "\n",
    "\n",
    "    try:\n",
    "        data = al.utils.get_factor_and_forward_returns(cs_df.stack(), pricing, quantiles=5, bins=None, periods=[1])\n",
    "        factor_data[sentiment] = data\n",
    "    except:\n",
    "        skipped_sentiments.append(sentiment)\n",
    "\n",
    "if skipped_sentiments:\n",
    "    print('\\nskipped the following sentiments:\\n{}'.format('\\n'join(skipped_sentiments)))\n",
    "factor_data[sentiments[0]].head()"
   ]
  },
  {
   "source": [
    "## 4.3 Alphalens Format with Unix Time\n",
    "\n",
    "The alphalens mean_return_by_quantile func requires unix timestamp to work (create a factor DF with unix time)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the index of the factor df to unix time\n",
    "unixt_factor_data = {\n",
    "    factor: data.set_index(pd.MultiIndex.from_tuples(\n",
    "        [(x.timestamp(), y) for x, y in data.index.values],\n",
    "        names=['date', 'asset']))\n",
    "    for factor, data in unixt_factor_data.items()}"
   ]
  },
  {
   "source": [
    "## 4.4 Factor Returns\n",
    "\n",
    "Visualize factor returns vs time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor_name, data in factor_data.items():\n",
    "    ls_factor_returns[factor_name] = al.performance.factor_returns(data).iloc[:, 0]\n",
    "\n",
    "(1 + ls_factor_returns).cumprod().plot()"
   ]
  },
  {
   "source": [
    "## 4.5 Basis Points Per Day per Quantile\n",
    "\n",
    "Look beyond factor weighted returns (should be monotonic in quantiles). Analyze basis points of the factor returns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor_name, data in unixt_factor_data.items():\n",
    "    qr_factor_returns[factor_name] = al.performance.mean_return_by_quantile(data)[0].iloc[:, 0]\n",
    "\n",
    "(10000*qr_factor_returns).plot.bar(\n",
    "    subplots=True,\n",
    "    sharey=True,\n",
    "    layout=(5,3),\n",
    "    figsize=(14, 14),\n",
    "    legend=False)"
   ]
  },
  {
   "source": [
    "## 4.6 Turnover Analysis\n",
    "\n",
    "Here we can analyze how stable the alphas are over time without doing a full on backtest.  This is meant to measure the period to period variance of the alpha factor using Factor Rank Autocorrelation (FRA)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_fra = pd.DataFrame()\n",
    "\n",
    "for factor, data in unixt_factor_data.items():\n",
    "    ls_fra[factor] = al.performance.factor_rank_autocorrelation(data)\n",
    "\n",
    "\n",
    "ls_fra.plot(title='Factor Rank Autocorrelation')"
   ]
  },
  {
   "source": [
    "## 4.7 Sharpe Ratio of Alphas\n",
    "\n",
    "Measuring the sharpe ratio(s) procuced by the extracted alphas. We are looking for sharpe > 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 252 trading days in a year\n",
    "daily_annualization_factor = np.sqrt(252)\n",
    "# caluclation of sharpe ratio\n",
    "(daily_annualization_factor * ls_factor_returns.mean() / ls_factor_returns.std()).round(2)"
   ]
  }
 ]
}